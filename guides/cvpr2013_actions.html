<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CVPR 2013 Accepted Papers</title>


<style>
/* CSS */


body {
	margin: 0;
	padding: 0;
	font-family: arial;
	background-color: #FCFAF1;
	/*background-color: #F6F3E5;*/
}
.as {
	font-size: 12px;
	color: #900;
}
.ts {
	font-weight: bold;
	font-size: 14px;
}
.tt {
	color: #009;
	font-size: 13px;
}
.keywords {
	font-size: 12px;
	color: #0000A0;
}
h1 {
	font-size: 20px;
	padding: 0;
	margin: 0;
}
img {
	max-width: 100%;
}
#titdiv {
	width: 100%;
	height: 130px;
	background-color: #C05F24;
	/*background-color: #840000;*/
	color: white;

	padding-top: 20px;
	padding-left: 5%;

	border-bottom: 1px solid #540000;
}

#maindiv {
	width: 970px;
	padding: 15px;
	margin-left: auto;
	margin-right: auto;

	border-left: solid 1px #D6D3C5;
	border-right: solid 1px #D6D3C5;

	background-color: white;
}

.apaper {
	margin-top: 25px;
	min-height: 300px;
}

.paperdesc {
	float: left;
}

.dllinks {
	float: right;
	text-align: right;
}

#titdiv a:link{ color: white; }
#titdiv a:visited{ color: white; }

#maindiv a:link{ color: #666; }
#maindiv a:visited{ color: #600; }

.t0 { color: #000;}
.t1 { color: #C00;}
.t2 { color: #0C0;}
.t3 { color: #00C;}
.t4 { color: #AA0;}
.t5 { color: #C0C;}
.t6 { color: #0CC;}

.topicchoice {
	border: 2px solid black;
	border-radius: 10px;
	padding: 4px;
	cursor: pointer;
	text-decoration: underline;
}

#explanation {
	/*background-color: #CFC;*/
	background-color: #f7a76f;
	border-radius: 5px;
	color: black;
	padding: 5px;
	text-align: center;
	font-weight: bold;
}

#sortoptions {
	text-align: center;
	padding: 10px;
}

.sim {
	cursor: pointer;
	text-decoration: underline;
}

.abstr {
	cursor: pointer;
	text-decoration: underline;
}

.abstrholder {
	background-color: #DFD;
	border: 1px solid #BDB;
	font-size: 12px;
	padding: 10px;
	border-radius: 5px;
	display: none; /* so that these are hidden initially */
	margin-bottom: 5px;
}

</style>

<script src="jquery-1.10.2.min.js"></script>
<script>

// this line below will get filled in with database of LDA topic distributions for top words
// for every paper
ldadist=[[0.18,0.09,0.15,0.10,0.20,0.13,0.15],[0.18,0.09,0.15,0.10,0.13,0.11,0.24],[0.24,0.10,0.14,0.12,0.13,0.13,0.13],[0.18,0.09,0.14,0.10,0.17,0.19,0.13],[0.17,0.25,0.13,0.10,0.11,0.10,0.13],[0.20,0.12,0.17,0.11,0.15,0.12,0.12],[0.12,0.10,0.15,0.11,0.12,0.22,0.17],[0.17,0.10,0.15,0.12,0.12,0.13,0.21],[0.20,0.15,0.14,0.11,0.12,0.12,0.19],[0.21,0.09,0.15,0.11,0.14,0.16,0.14],[0.19,0.22,0.11,0.14,0.14,0.10,0.11],[0.25,0.12,0.12,0.11,0.12,0.11,0.15],[0.14,0.14,0.15,0.20,0.11,0.13,0.14],[0.24,0.11,0.14,0.10,0.14,0.13,0.14],[0.18,0.11,0.15,0.10,0.22,0.12,0.13],[0.18,0.12,0.14,0.09,0.21,0.12,0.14],[0.18,0.10,0.15,0.10,0.20,0.13,0.15],[0.17,0.10,0.22,0.11,0.14,0.11,0.15],[0.25,0.12,0.14,0.11,0.14,0.10,0.14],[0.16,0.12,0.13,0.10,0.15,0.12,0.21],[0.15,0.11,0.21,0.12,0.15,0.14,0.13],[0.17,0.12,0.14,0.18,0.15,0.14,0.11],[0.14,0.10,0.23,0.12,0.14,0.13,0.14],[0.18,0.21,0.14,0.10,0.14,0.11,0.12],[0.14,0.13,0.15,0.13,0.20,0.14,0.11],[0.18,0.11,0.14,0.16,0.12,0.14,0.15],[0.22,0.08,0.16,0.12,0.15,0.13,0.14],[0.20,0.11,0.15,0.10,0.19,0.13,0.13],[0.18,0.11,0.14,0.12,0.13,0.12,0.20],[0.17,0.09,0.16,0.11,0.13,0.18,0.15],[0.18,0.12,0.13,0.18,0.12,0.12,0.15],[0.22,0.11,0.14,0.11,0.15,0.12,0.14]]

// this will be filled with pairwise scores between papers
pairdists=[[260.11,25.58,19.80,32.45,12.85,30.76,29.79,16.29,21.77,38.01,27.85,20.87,14.15,39.00,32.05,37.43,36.56,14.90,28.42,28.70,29.16,21.31,28.67,30.84,25.86,19.46,42.94,31.66,23.97,33.43,11.31,35.19],[25.58,318.00,33.71,23.29,16.11,18.15,21.85,27.88,23.50,18.93,10.24,30.99,17.06,30.35,21.80,18.66,30.26,21.09,23.14,32.35,22.29,9.06,21.40,23.76,14.44,21.95,20.76,6.42,13.81,21.56,11.34,19.64],[19.80,33.71,269.08,22.38,23.46,23.14,32.44,29.48,33.17,48.45,14.20,47.09,13.70,21.07,18.17,14.92,22.30,25.09,21.72,20.25,33.46,17.63,21.29,25.17,18.04,26.36,39.77,23.61,15.94,26.56,28.85,34.16],[32.45,23.29,22.38,286.59,14.41,32.30,15.19,22.83,14.02,30.14,15.82,26.38,8.99,23.38,32.02,37.02,50.63,15.26,11.35,13.55,28.09,16.73,31.52,18.54,15.87,23.07,29.82,32.65,17.73,18.65,23.01,40.64],[12.85,16.11,23.46,14.41,285.49,32.76,17.59,27.31,45.01,14.56,21.22,38.12,11.62,22.62,13.59,22.86,15.25,20.24,26.47,23.75,23.02,28.77,22.10,57.24,23.94,29.71,18.56,26.27,37.24,16.06,20.22,23.98],[30.76,18.15,23.14,32.30,32.76,266.02,17.29,25.95,24.12,27.95,14.10,38.95,11.67,27.95,30.66,26.97,24.64,24.79,29.14,24.13,25.25,19.61,20.15,32.12,22.43,27.03,30.72,34.11,29.73,18.18,14.87,45.14],[29.79,21.85,32.44,15.19,17.59,17.29,314.64,27.61,29.81,34.16,10.30,27.55,24.04,22.19,17.72,15.53,8.85,24.59,13.72,23.62,18.28,25.04,20.08,25.71,21.74,28.49,26.42,27.02,15.86,27.89,25.49,18.88],[16.29,27.88,29.48,22.83,27.31,25.95,27.61,254.54,35.90,29.83,25.87,33.64,17.85,19.01,23.27,18.96,22.65,43.78,25.93,33.60,42.58,26.71,26.00,25.50,20.08,24.65,29.19,17.52,35.19,44.30,32.17,28.95],[21.77,23.50,33.17,14.02,45.01,24.12,29.81,35.90,272.68,37.45,13.88,45.13,17.13,28.16,25.49,25.17,26.73,20.60,23.93,21.28,27.67,28.99,14.20,26.36,13.18,24.97,31.47,12.26,30.34,24.36,32.36,16.79],[38.01,18.93,48.45,30.14,14.56,27.95,34.16,29.83,37.45,246.59,28.86,46.33,18.28,34.89,21.26,21.14,30.95,21.90,30.93,20.08,28.50,34.68,28.36,24.09,21.36,32.06,43.50,23.38,19.91,31.72,23.41,30.09],[27.85,10.24,14.20,15.82,21.22,14.10,10.30,25.87,13.88,28.86,311.33,26.94,30.06,38.54,20.87,17.26,28.60,7.21,38.50,18.90,21.45,36.27,21.19,21.09,17.72,19.61,22.91,20.73,12.00,20.36,24.03,22.90],[20.87,30.99,47.09,26.38,38.12,38.95,27.55,33.64,45.13,46.33,26.94,241.66,20.14,37.51,24.93,26.77,20.51,27.02,37.87,25.29,22.32,26.88,25.08,32.34,13.91,25.61,51.79,41.39,17.62,19.59,28.82,24.90],[14.15,17.06,13.70,8.99,11.62,11.67,24.04,17.85,17.13,18.28,30.06,20.14,325.73,15.96,20.45,17.90,14.59,27.37,28.09,20.94,18.31,31.87,28.21,11.52,10.60,18.16,23.09,10.71,28.68,26.23,15.91,19.57],[39.00,30.35,21.07,23.38,22.62,27.95,22.19,19.01,28.16,34.89,38.54,37.51,15.96,270.14,27.11,25.67,33.09,18.41,43.80,22.96,19.57,15.51,25.90,18.08,21.98,21.92,23.80,29.07,21.40,20.67,18.42,23.96],[32.05,21.80,18.17,32.02,13.59,30.66,17.72,23.27,25.49,21.26,20.87,24.93,20.45,27.11,280.82,41.78,37.61,22.66,21.63,31.99,31.14,18.00,20.25,27.26,23.97,18.26,29.67,44.07,16.41,24.19,19.03,26.56],[37.43,18.66,14.92,37.02,22.86,26.97,15.53,18.96,25.17,21.14,17.26,26.77,17.90,25.67,41.78,278.20,27.24,13.44,18.43,26.64,33.96,22.88,29.26,33.24,30.53,10.09,25.24,53.80,19.15,12.66,9.31,33.56],[36.56,30.26,22.30,50.63,15.25,24.64,8.85,22.65,26.73,30.95,28.60,20.51,14.59,33.09,37.61,27.24,270.78,17.07,19.91,31.68,21.91,14.66,22.80,26.88,18.78,26.69,21.48,37.33,26.46,17.07,28.38,45.22],[14.90,21.09,25.09,15.26,20.24,24.79,24.59,43.78,20.60,21.90,7.21,27.02,27.37,18.41,22.66,13.44,17.07,289.81,24.40,17.20,29.35,21.12,27.02,29.86,25.33,28.01,30.67,28.01,24.42,25.57,32.18,25.43],[28.42,23.14,21.72,11.35,26.47,29.14,13.72,25.93,23.93,30.93,38.50,37.87,28.09,43.80,21.63,18.43,19.91,24.40,285.27,17.95,19.77,16.01,28.06,20.99,14.54,24.85,41.18,23.26,31.90,21.86,15.18,17.82],[28.70,32.35,20.25,13.55,23.75,24.13,23.62,33.60,21.28,20.08,18.90,25.29,20.94,22.96,31.99,26.64,31.68,17.20,17.95,293.53,30.26,26.46,35.56,27.99,18.87,15.14,21.17,19.36,20.11,18.65,27.57,27.11],[29.16,22.29,33.46,28.09,23.02,25.25,18.28,42.58,27.67,28.50,21.45,22.32,18.31,19.57,31.14,33.96,21.91,29.35,19.77,30.26,266.68,39.61,26.27,37.92,26.51,33.61,37.31,28.84,14.19,39.15,14.32,28.96],[21.31,9.06,17.63,16.73,28.77,19.61,25.04,26.71,28.99,34.68,36.27,26.88,31.87,15.51,18.00,22.88,14.66,21.12,16.01,26.46,39.61,294.21,25.89,22.90,38.14,24.99,33.62,21.24,18.40,17.94,16.02,21.33],[28.67,21.40,21.29,31.52,22.10,20.15,20.08,26.00,14.20,28.36,21.19,25.08,28.21,25.90,20.25,29.26,22.80,27.02,28.06,35.56,26.27,25.89,291.37,25.31,18.36,24.58,42.04,17.66,14.89,35.32,21.42,24.58],[30.84,23.76,25.17,18.54,57.24,32.12,25.71,25.50,26.36,24.09,21.09,32.34,11.52,18.08,27.26,33.24,26.88,29.86,20.99,27.99,37.92,22.90,25.31,270.66,20.51,29.13,22.90,32.21,16.91,15.64,7.62,41.52],[25.86,14.44,18.04,15.87,23.94,22.43,21.74,20.08,13.18,21.36,17.72,13.91,10.60,21.98,23.97,30.53,18.78,25.33,14.54,18.87,26.51,38.14,18.36,20.51,325.65,23.15,22.72,15.01,4.72,24.35,21.80,18.99],[19.46,21.95,26.36,23.07,29.71,27.03,28.49,24.65,24.97,32.06,19.61,25.61,18.16,21.92,18.26,10.09,26.69,28.01,24.85,15.14,33.61,24.99,24.58,29.13,23.15,284.64,16.46,15.64,42.62,33.43,17.72,26.04],[42.94,20.76,39.77,29.82,18.56,30.72,26.42,29.19,31.47,43.50,22.91,51.79,23.09,23.80,29.67,25.24,21.48,30.67,41.18,21.17,37.31,33.62,42.04,22.90,22.72,16.46,249.66,37.79,12.98,31.53,22.34,34.78],[31.66,6.42,23.61,32.65,26.27,34.11,27.02,17.52,12.26,23.38,20.73,41.39,10.71,29.07,44.07,53.80,37.33,28.01,23.26,19.36,28.84,21.24,17.66,32.21,15.01,15.64,37.79,262.32,18.68,29.63,26.67,33.02],[23.97,13.81,15.94,17.73,37.24,29.73,15.86,35.19,30.34,19.91,12.00,17.62,28.68,21.40,16.41,19.15,26.46,24.42,31.90,20.11,14.19,18.40,14.89,16.91,4.72,42.62,12.98,18.68,305.66,23.72,23.05,23.74],[33.43,21.56,26.56,18.65,16.06,18.18,27.89,44.30,24.36,31.72,20.36,19.59,26.23,20.67,24.19,12.66,17.07,25.57,21.86,18.65,39.15,17.94,35.32,15.64,24.35,33.43,31.53,29.63,23.72,282.90,20.76,23.19],[11.31,11.34,28.85,23.01,20.22,14.87,25.49,32.17,32.36,23.41,24.03,28.82,15.91,18.42,19.03,9.31,28.38,32.18,15.18,27.57,14.32,16.02,21.42,7.62,21.80,17.72,22.34,26.67,23.05,20.76,306.10,18.59],[35.19,19.64,34.16,40.64,23.98,45.14,18.88,28.95,16.79,30.09,22.90,24.90,19.57,23.96,26.56,33.56,45.22,25.43,17.82,27.11,28.96,21.33,24.58,41.52,18.99,26.04,34.78,33.02,23.74,23.19,18.59,254.78]]

var choices = [0, 0, 0, 1, 1, 0, 0]; // default choices, random...
var similarityMode = 0; // is the user currently looking at papers similar to some one paper?
var similarTo = 0; // the index of query paper

// given choices of topics to sort by, handle user interface stuff (i.e. show selection)
function colorChoices() {
	for(var i=0;i<choices.length;i++) {
		if(choices[i] == 1) {
			$("#tc"+i).css("background-color", "#EFE");
			$("#tc"+i).css("border-color", "#575");
		} else {
			$("#tc"+i).css("background-color", "#FFF");
			$("#tc"+i).css("border-color", "#FFF");
		}
	}
}

// this permutes the divs (that contian 1 paper each) based on a custom sorting function
// in our case, this sort is done as dot product based on the choices[] array
// here we are guaranteed ldadist[] already sums to 1 for every paper
function arrangeDivs() {
	var rtable = $("#rtable");
	var paperdivs = rtable.children(".apaper");

	// normalize choices to sum to 1
	var nn = choices.slice(0); // copy the array
	var ss = 0.0;
	for(var j=0;j<choices.length;j++) { ss += choices[j]; }
	for(var j=0;j<choices.length;j++) { nn[j] = nn[j]/ss; }

	paperdivs.detach().sort(function(a,b) {
		var ixa = parseInt($(a).attr('id').substring(3));
		var ixb = parseInt($(b).attr('id').substring(3));

		if(similarityMode === 1) {
			return pairdists[ixa][similarTo] < pairdists[ixb][similarTo] ? 1 : -1;
		}

		if(similarityMode === 0) {

			// chi-squared kernel for the two histograms
			var accuma = 0;
			var accumb = 0;
			for(var i=0;i<7;i++) {
				var ai= ldadist[ixa][i];
				var bi= ldadist[ixb][i];
				var ci= choices[i];
				accuma += (ai-ci)*(ai-ci)/(ai+ci);
				accumb += (bi-ci)*(bi-ci)/(bi+ci);
			}
			return accuma > accumb ? 1 : -1;

			/*
			// vector distance. These are histograms... but lets pretend they arent
			var accuma = 0;
			var accumb = 0;
			for(var i=0;i<7;i++) {
				var ai= ldadist[ixa][i];
				var bi= ldadist[ixb][i];
				var ci= nn[i];
				accuma += (ai-ci)*(ai-ci);
				accumb += (bi-ci)*(bi-ci);
			}
			return accuma > accumb ? 1 : -1;
			*/

			/*
			// inner product distance
			var accuma = 0;
			var accumb = 0;
			for(var i=0;i<7;i++) {
				accuma += ldadist[ixa][i] * choices[i];
				accumb += ldadist[ixb][i] * choices[i];
			}
			return accuma < accumb ? 1 : -1;
			*/
		}

	});
	rtable.append(paperdivs);
}

// when page loads...
$(document).ready(function(){

	arrangeDivs();
	colorChoices();

	// user clicks on one of the Topic buttons
	$(".topicchoice").click(function() {
		similarityMode = 0; // make sure this is off
		var tcid = parseInt($(this).attr('id').substring(2));
		choices[tcid] = 1 - choices[tcid]; // toggle!

		colorChoices();
		arrangeDivs();
	});

	// user clicks on "rank by tf-idf similarity to this" button for some paper
	$(".sim").click(function() {
		similarityMode = 1; // turn on similarity mode
		for(var i=0;i<choices.length;i++) { choices[i] = 0; } // zero out choices
		similarTo = parseInt($(this).attr('id').substring(3)); // store id of the paper clicked

		colorChoices();
		arrangeDivs();

		// also scroll to top
		$('html, body').animate({ scrollTop: 0 }, 'fast');
	});

	// user clicks on "abstract button for some paper
	$(".abstr").click(function() {
		var pid = parseInt($(this).attr('id').substring(2)); // id of the paper clicked
		var aurl = "abstracts/a" + pid + ".txt";
		var holderdiv = "#abholder" + pid;

		if($(holderdiv).is(':visible')) {

			$(holderdiv).slideUp(); // hide the abstract away

		} else {

			// do ajax request and fill the abstract div with the result
			$.ajax({
	            url : aurl,
	            dataType: "text",
	            success : function (data) {
	                $(holderdiv).html(data);
	                $(holderdiv).slideDown();
	            }
	        });
		}
	});
});

</script>

</head>

<body>

<div id ="titdiv">
<h1>CVPR 2013 - Action Recognition Papers</h1>
maintained by <a href="http://www.colinlea.com">Colin Lea</a><br/>
adapted from <a href="https://twitter.com/karpathy">@karpathy</a>'s <a href="http://cs.stanford.edu/people/karpathy/nipspreview/">NIPS 2012 guide</a> <br/> <span style="padding-left:67px"> and <a href="https://twitter.com/benhamner">@benhamner</a>'s <a href="http://benhamner.com/icml2013preview/">ICML 2013 guide</a></span><br/>
source code on <a href="https://github.com/colincsl/cvpr2013papers">github</a><br />
data from <a href="http://www.cv-foundation.org/openaccess/CVPR2013.py">here</a>
</div>

<div id="maindiv">
<!-- <div id="explanation">Below every paper are the 100 most frequently used words in that paper. The color is based on an LDA topic model with k = 7.<br/>  -->
<div id="explanation">The 100 most frequent words in each paper are listed below each entry. Color is based on an LDA topic model with k = 7.<br />
	<!-- <div style="font-size: 14px;">(Roughly, topic 0 = discriminative, 1 = multimodal, 2 = structure/hierarchy, 3 = interactions, 4 = features, 5 = sequences, 6 = attributes) </div> -->
</div>
<div id="sortoptions">

<br/>
<b>Notes</b>: the topics here are much more vague than for the <a href="colinlea.com/guides/cvpr2013.html">clustering of all CVPR2013</a> papers.<br/>
For an overview of activity recognition at CVPR13 <a href="https://colinlea.wordpress.com/2013/07/12/cvpr2013-activity-recognition/">see my blog post</a>

<br/><br/>
<!-- Toggle LDA topics to sort by: -->
Toggle the topics below to sort by (rough) category:<br/><br/>

<span class="topicchoice t0" id="tc0">(0) discriminative</span>
<span class="topicchoice t1" id="tc1">(1) multimodal</span>
<span class="topicchoice t2" id="tc2">(2) structure/hierarchy</span>
<span class="topicchoice t3" id="tc3">(3) interactions</span>
<span class="topicchoice t4" id="tc4">(4) features</span>
<span class="topicchoice t5" id="tc5">(5) sequences</span>
<span class="topicchoice t6" id="tc6">(6) attributes</span>
</div>

<!-- the keyword below will be replaced by content from the python script generatenice.py -->
<div id="rtable">


	<div class="apaper" id="pid0">
	<div class="paperdesc">
		<span class="ts">HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences</span><br />
		<span class="as">Omar Oreifej, Zicheng Liu</span><br />
		<span class="keywords">Keywords: Activity Recognition, Kinect, Action Recognition, Histogram of Gradients, HOG, Histogram of Normals, HON, 4D Normals, Polychoron, MSR Action 3D, MSR Action Pairs, MSR Daily Activity, Depth, Shape, 4D, 3D</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Oreifej_HON4D_Histogram_of_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim0">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab522678274">[abstract]</span>-->
	</div>
	<img src = "thumbs/Oreifej_HON4D_Histogram_of_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder522678274"></div>-->
	<span class="tt">[<span class="t0">action, discriminative, space, box, spatiotemporal, distribution, accuracy, capture, human, figure, joint, feature, gesture, holistic, svm, pyramid, training</span>] [<span class="t1">shape, corresponding, note, consider</span>] [<span class="t2">recognition, previous, proposed, spatial, video, oriented, global</span>] [<span class="t3">sequence, number, unit</span>] [<span class="t4">depth, surface, normal, motion, orientation, histogram, descriptor, dataset, compute, order, gradient, skeleton, density, msr, local, computed, quantization, chair, poster, backpack, hat, lop, table, interest, jiang, occupancy, random, described, quantize, higher, compared, dollar, hog, better, place, outperforms</span>] [<span class="t5">temporal, experiment, based, color, selected, performance, regular</span>] [<span class="t6">activity, method, set, example, object, hand, compare</span>] </span>
	</div>



	<div class="apaper" id="pid1">
	<div class="paperdesc">
		<span class="ts">Modeling Actions through State Changes</span><br />
		<span class="as">Alireza Fathi, James M. Rehg</span><br />
		<span class="keywords">Keywords: Action Recognition, State, Object, Smi-Supervised Learning, Egocentric</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Fathi_Modeling_Actions_through_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim1">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab202215044">[abstract]</span>-->
	</div>
	<img src = "thumbs/Fathi_Modeling_Actions_through_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder202215044"></div>-->
	<span class="tt">[<span class="t0">action, model, training, svm, accuracy, figure, trained, feature, class, detection</span>] [<span class="t1">linear, corresponding, objective, learn</span>] [<span class="t2">recognition, video, previous, represent, test, segment, cluster</span>] [<span class="t3">describe, second, sequence</span>] [<span class="t4">order, performed, bag, vector, motion</span>] [<span class="t5">based, detector, result</span>] [<span class="t6">state, method, object, activity, coffee, open, region, set, frame, change, bread, recognizing, close, spoon, segmentation, pour, correspond, final, initial, interval, compare, fig, cup, environment, material, appearance, response, daily, train, introduce, instance, build, jelly, recognize, consistently, water, classifier, cheese, sugar, hand, spread, iien, iist, peanut, consistent, changed, apply, scoop, ffe, identifies, jar, jam</span>] </span>
	</div>



	<div class="apaper" id="pid2">
	<div class="paperdesc">
		<span class="ts">Representing Videos Using Mid-level Discriminative Patches</span><br />
		<span class="as">Arpit Jain, Abhinav Gupta, Mikel Rodriguez, Larry S. Davis</span><br />
		<span class="keywords">Keywords: Action Recognition, Video Understanding</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Jain_Representing_Videos_Using_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim2">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab747343624">[abstract]</span>-->
	</div>
	<img src = "thumbs/Jain_Representing_Videos_Using_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder747343624"></div>-->
	<span class="tt">[<span class="t0">action, discriminative, training, class, approach, figure, human, select, transfer, patch, detection, feature, correspondence, vocabulary, strong, establish, olympics, representative, svm, spatiotemporal, simple, align, cost, establishing, score, euclidean, pose, trained, propose, ipfp, localization, representing, partition</span>] [<span class="t1">data, learn, demonstrate, algorithm</span>] [<span class="t2">video, test, recognition, represent, cluster, alignment, clustering, global, evaluate, throw</span>] [<span class="t3">learning, semantic, number, top, function, nearest, constituent</span>] [<span class="t4">motion, dataset, table, vector, query, background</span>] [<span class="t5">representation, based, performance, selected, term, selection, temporal, event, evaluation, perform, candidate</span>] [<span class="t6">object, label, distance, set, work, recognizing, consistent, appearance</span>] </span>
	</div>



	<div class="apaper" id="pid3">
	<div class="paperdesc">
		<span class="ts">Evaluation of Color STIPs for Human Action Recognition</span><br />
		<span class="as">Ivo Everts, Jan C. van_Gemert, Theo Gevers</span><br />
		<span class="keywords">Keywords: action recognition, color, evaluation</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Everts_Evaluation_of_Color_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim3">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab659402510">[abstract]</span>-->
	</div>
	<img src = "thumbs/Everts_Evaluation_of_Color_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder659402510"></div>-->
	<span class="tt">[<span class="t0">action, human, discriminative, detection, ucf, accuracy, feature</span>] [<span class="t1">image, consider, single, represented, note</span>] [<span class="t2">recognition, video, codebook, achieved, spatial, scene</span>] [<span class="t3">level, function, number</span>] [<span class="t4">descriptor, gradient, local, table, dataset, extracted, best, higher, motion, scale, vector, average</span>] [<span class="t5">stip, color, stips, photometric, gabor, performance, based, harris, intensity, chromatic, representation, temporal, associated, detector, invariant, repeatability, combination, evaluation, energy, considering, reported, channel, typically, invariance, signal, dimensionality, full, outperform, differential, result, tensor, integration, power, opponent, focus, opposing, bank, quality</span>] [<span class="t6">recognizing, object, frame, response, set</span>] </span>
	</div>



	<div class="apaper" id="pid4">
	<div class="paperdesc">
		<span class="ts">Event Recognition in Videos by Learning from Heterogeneous Web Sources</span><br />
		<span class="as">Lin Chen, Lixin Duan, Dong Xu</span><br />
		<span class="keywords">Keywords: Domain Adaptation, Event Recognition</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Chen_Event_Recognition_in_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim4">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab433244944">[abstract]</span>-->
	</div>
	<img src = "thumbs/Chen_Event_Recognition_in_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder433244944"></div>-->
	<span class="tt">[<span class="t0">training, svm, feature, propose, model, action, trained, human</span>] [<span class="t1">domain, source, target, data, adaptation, web, consumer, problem, heterogeneous, multiple, labeled, view, image, optimization, learn, existing, optimal, unlabeled, ccv, algorithm, setting, youtube, represented, visual, solving, note, dso, objective, learnt, kodak, regularizer, max, type, sift, mkl, sample, single, matrix, assume, effectively, cope, bing, effectiveness, infer, solve, convex, leveraging</span>] [<span class="t2">recognition, video, kernel, proposed, task, simultaneously</span>] [<span class="t3">learning, number, relevant, three, function</span>] [<span class="t4">dataset, vector, large, datasets, report</span>] [<span class="t5">event, based, performance</span>] [<span class="t6">method, set, work, weight, label, well, min</span>] </span>
	</div>



	<div class="apaper" id="pid5">
	<div class="paperdesc">
		<span class="ts">Multi-task Sparse Learning with Beta Process Prior for Action Recognition</span><br />
		<span class="as">Chunfeng Yuan, Weiming Hu, Guodong Tian, Shuang Yang, Haoran Wang</span><br />
		<span class="keywords">Keywords: </span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yuan_Multi-task_Sparse_Learning_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim5">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab185806097">[abstract]</span>-->
	</div>
	<img src = "thumbs/Yuan_Multi-task_Sparse_Learning_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder185806097"></div>-->
	<span class="tt">[<span class="t0">action, feature, model, human, training, ucf, kth, error, svm, class, approach, joint, distribution, figure, improve, employ, discriminative, propose</span>] [<span class="t1">single, sample, visual, multiple, data, image, sift, matrix</span>] [<span class="t2">sparse, dictionary, task, recognition, video, proposed, sparsity, beta, prior, process, mtsl, test, posterior, learned, bayesian, represent, reconstruction, residual, common, geometric, symbolized, follow, shared, jth, swinging, gibbs, gamma, norm, expressed</span>] [<span class="t3">learning, individual, function, level, number</span>] [<span class="t4">local, histogram, sampling, table, vector, comparison, compared, descriptor, performed, average, combined, interest, density, listed</span>] [<span class="t5">representation, combination, based, associated, performance, formulation, total</span>] [<span class="t6">method, set, initial, jointly, inference</span>] </span>
	</div>



	<div class="apaper" id="pid6">
	<div class="paperdesc">
		<span class="ts">Story-Driven Summarization for Egocentric Video</span><br />
		<span class="as">Zheng Lu, Kristen Grauman</span><br />
		<span class="keywords">Keywords: video summarization, egocentric, story</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Lu_Story-Driven_Summarization_for_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim6">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab274713476">[abstract]</span>-->
	</div>
	<img src = "thumbs/Lu_Story-Driven_Summarization_for_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder274713476"></div>-->
	<span class="tt">[<span class="t0">approach, figure, score, capture, select, model</span>] [<span class="t1">visual, camera, objective, multiple, data, path</span>] [<span class="t2">video, scene, baseline, long</span>] [<span class="t3">summarization, input, three, number, measure, individual, text</span>] [<span class="t4">sampling, motion, compute, computed, random</span>] [<span class="t5">subshots, egocentric, chain, subshot, summary, story, selected, diversity, user, based, novel, uniform, total, good, news, adl, ute, term, event, quality, selection, metric, study, queue, making, priority, progress, color, coherent, keyframes, connecting, include, activation, temporal, generate, boundary, key, candidate, selecting, approximate, form</span>] [<span class="t6">method, object, set, work, original, graph, segmentation, well, activity, frame, daily, example, pattern</span>] </span>
	</div>



	<div class="apaper" id="pid7">
	<div class="paperdesc">
		<span class="ts">Context-Aware Modeling and Recognition of Activities in Video</span><br />
		<span class="as">Yingying Zhu, Nandita M. Nayak, Amit K. Roy-Chowdhury</span><br />
		<span class="keywords">Keywords: </span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zhu_Context-Aware_Modeling_and_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim7">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab355749020">[abstract]</span>-->
	</div>
	<img src = "thumbs/Zhu_Context-Aware_Modeling_and_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder355749020"></div>-->
	<span class="tt">[<span class="t0">model, action, feature, figure, approach, human, space, training</span>] [<span class="t1">algorithm, continuous, problem, framework, image, matrix</span>] [<span class="t2">recognition, baseline, spatial, video, recall, precision, proposed, learned, scene</span>] [<span class="t3">detected, time, number, agent, learning, function, group, duration</span>] [<span class="t4">motion, vector, interest, dataset, complex, size</span>] [<span class="t5">temporal, modeling, based, representation, event</span>] [<span class="t6">activity, context, person, release, virat, structural, method, frame, defined, region, object, segmentation, inference, nth, work, example, classifier, moving, vehicle, label, potential, greedy, distance, set, search, optimum, recognized, tcij, scij, graph, jointly, normalized, compatibility, developed, recognizing, computer, weight, incorrectly</span>] </span>
	</div>



	<div class="apaper" id="pid8">
	<div class="paperdesc">
		<span class="ts">Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</span><br />
		<span class="as">Chao-Yeh Chen, Kristen Grauman</span><br />
		<span class="keywords">Keywords: </span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Chen_Watching_Unlabeled_Video_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim8">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab536368293">[abstract]</span>-->
	</div>
	<img src = "thumbs/Chen_Watching_Unlabeled_Video_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder536368293"></div>-->
	<span class="tt">[<span class="t0">training, pose, action, human, body, figure, approach, accuracy, space, class, stanford, riding, svm, feature, bounding</span>] [<span class="t1">unlabeled, data, labeled, domain, adaptation, image, learn, source, people, note</span>] [<span class="t2">video, recognition, test, prior, idea, baseline, learned</span>] [<span class="t3">learning, system, additional, number, small, nearest, annotation, relevant, function</span>] [<span class="t4">motion, real, dataset, hollywood, impact, datasets</span>] [<span class="t5">novel, representation, based, poselet, account, generate, generic, poselets</span>] [<span class="t6">set, synthetic, static, method, activity, manifold, original, pav, pascal, object, person, neighbor, train, label, augment, work, extrapolate, appearance, map, category, denote, expand, lle, nonlinear, explore, example, well</span>] </span>
	</div>



	<div class="apaper" id="pid9">
	<div class="paperdesc">
		<span class="ts">Poselet Key-Framing: A Model for Human Activity Recognition</span><br />
		<span class="as">Michalis Raptis, Leonid Sigal</span><br />
		<span class="keywords">Keywords: Activity Recognition, Discriminative Keyframes, Video Analysis</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Raptis_Poselet_Key-Framing_A_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim9">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab435255560">[abstract]</span>-->
	</div>
	<img src = "thumbs/Raptis_Poselet_Key-Framing_A_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder435255560"></div>-->
	<span class="tt">[<span class="t0">model, action, human, discriminative, detection, pose, localization, bounding, training, accuracy, scoring, score, approach, capture, box, figure, feature, propose, holistic</span>] [<span class="t1">framework, image, algorithm</span>] [<span class="t2">video, recognition, proposed, spatial, test, structure, dynamic, indicates</span>] [<span class="t3">latent, learning, pairwise, function, number, positive, sequence, detected, rate, semantic, second</span>] [<span class="t4">hog, local, average, motion, table, descriptor</span>] [<span class="t5">temporal, keyframes, poselets, poselet, unary, representation, based, bow, keyframe, performance, approachperson, partial, key, entire, selected, legsextended, activation, modeling, temporally, streaming, rely, focus, allows, spatially, selection, ability, localize</span>] [<span class="t6">set, activity, frame, method, static, recognizing</span>] </span>
	</div>



	<div class="apaper" id="pid10">
	<div class="paperdesc">
		<span class="ts">3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image</span><br />
		<span class="as">Ishani Chakraborty, Hui Cheng, Omar Javed</span><br />
		<span class="keywords">Keywords: Visual Proxemics, 3D people layout, semantic constraints, RANSAC</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Chakraborty_3D_Visual_Proxemics_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim10">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab398905781">[abstract]</span>-->
	</div>
	<img src = "thumbs/Chakraborty_3D_Visual_Proxemics_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder398905781"></div>-->
	<span class="tt">[<span class="t0">model, figure, human, pose, estimation, feature, space, detection, approach, capture</span>] [<span class="t1">camera, people, face, visual, height, image, proxemics, layout, outlier, estimate, shape, consider, constraint, ransac, single, shot, ground, crowd, perspective, linear, plane, proxemes, framework, proxeme, robust, position, photo, estimated, visibility, center, family, location, audience, horizon, rectification, classification, type, derived, typical</span>] [<span class="t2">spatial, proposed, indicates</span>] [<span class="t3">group, interaction, social, semantic, number, analysis, detected, detect, level</span>] [<span class="t4">depth, table, described, size, average, compute, dataset, vector, better, standard</span>] [<span class="t5">based, performance</span>] [<span class="t6">set, distance, method</span>] </span>
	</div>



	<div class="apaper" id="pid11">
	<div class="paperdesc">
		<span class="ts">Expanded Parts Model for Human Attribute and Action Recognition in Still Images</span><br />
		<span class="as">Gaurav Sharma, Frdric Jurie, Cordelia Schmid</span><br />
		<span class="keywords">Keywords: human analysis, part-based, discriminative, margin maximization, expanded parts, attributes, human</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Sharma_Expanded_Parts_Model_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim11">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab288222902">[abstract]</span>-->
	</div>
	<img src = "thumbs/Sharma_Expanded_Parts_Model_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder288222902"></div>-->
	<span class="tt">[<span class="t0">human, model, discriminative, training, scoring, action, pose, class, bof, feature, propose, score, spm, figure, willow, patch, stanford, expanded, current, attribute, explain, approach, detection, estimation, qualitative, validation, bounding, template, trained, distribution</span>] [<span class="t1">image, learnt, learn, corresponding, algorithm, data, linear, optimization, sift</span>] [<span class="t2">recognition, baseline, spatial, proposed, norm, partially, test</span>] [<span class="t3">learning, number, function, high, positive, collection, three, row, latent</span>] [<span class="t4">database, large, better, best, sampling, dataset, vector</span>] [<span class="t5">based, performance, full, representation, loss, good, associated, component</span>] [<span class="t6">method, object, context, map, work, appearance, train, recognizing, set, well, stochastic, example</span>] </span>
	</div>



	<div class="apaper" id="pid12">
	<div class="paperdesc">
		<span class="ts">Decoding Children's Social Behavior</span><br />
		<span class="as">James M. Rehg, Gregory D. Abowd, Agata Rozga, Mario Romero, Mark A. Clements, Stan Sclaroff, Irfan Essa, Opal Y. Ousley, Yin Li, Chanho Kim, Hrishikesh Rao, Jonathan C. Kim, Liliana Lo Presti, Jianming Zhang, Denis Lantsman, Jonathan Bidwell, Zhefan Ye</span><br />
		<span class="keywords">Keywords: </span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Rehg_Decoding_Childrens_Social_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim12">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab650135107">[abstract]</span>-->
	</div>
	<img src = "thumbs/Rehg_Decoding_Childrens_Social_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder650135107"></div>-->
	<span class="tt">[<span class="t0">detection, feature, accuracy, approach, training, error, trained, figure, score</span>] [<span class="t1">face, estimate, camera, domain</span>] [<span class="t2">behavior, video, recognition, detecting, prediction, clip</span>] [<span class="t3">child, ball, social, examiner, book, play, analysis, interaction, engagement, audio, gaze, stage, smile, number, dyadic, touched, adult, eye, experimental, speech, tracker, structured, additional, head, multimodal, tickle, decoding, detected, parsing, detect, duration, true, autism, predicting, tool, predicted, developmental, georgia, discrete, degree, positive, measure, mmdb, false, testing, kinect, identify</span>] [<span class="t4">table, dataset, order, extracted, combined, hat</span>] [<span class="t5">based, event, key, assessment, performance</span>] [<span class="t6">activity, set, goal, work, tracking, introduce, predict, object, attention, context</span>] </span>
	</div>



	<div class="apaper" id="pid13">
	<div class="paperdesc">
		<span class="ts">An Approach to Pose-Based Action Recognition</span><br />
		<span class="as">Chunyu Wang, Yizhou Wang, Alan L. Yuille</span><br />
		<span class="keywords">Keywords: pose estimation, action recognition, feature learning</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wang_An_Approach_to_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim13">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab310583350">[abstract]</span>-->
	</div>
	<img src = "thumbs/Wang_An_Approach_to_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder310583350"></div>-->
	<span class="tt">[<span class="t0">pose, action, body, joint, human, figure, estimation, mining, approach, gesture, keck, model, accuracy, ucf, training, holistic, sport, transaction, left, contrast, discriminative, art, capture, frequently, call, challenging, turn, going, mine, growth, compose, pursue</span>] [<span class="t1">data, image, estimated, represented, camera, extract, estimate, location, learn</span>] [<span class="t2">recognition, video, proposed, spatial, represent, dictionary, intersection</span>] [<span class="t3">three, rate, high, detect</span>] [<span class="t4">motion, local, dataset, support, table, comparison, described, vector</span>] [<span class="t5">temporal, performance, based, representation, color, quantized</span>] [<span class="t6">method, set, state, work, frame, appearance, apply, compare</span>] </span>
	</div>



	<div class="apaper" id="pid14">
	<div class="paperdesc">
		<span class="ts">Hollywood 3D: Recognizing Actions in 3D Natural Scenes</span><br />
		<span class="as">Simon Hadfield, Richard Bowden</span><br />
		<span class="keywords">Keywords: action recognition, 3d, actions, hollywood, depth, stereo, 3.5d, 4d, interest points</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Hadfield_Hollywood_3D_Recognizing_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim14">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab288387063">[abstract]</span>-->
	</div>
	<img src = "thumbs/Hadfield_Hollywood_3D_Recognizing_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder288387063"></div>-->
	<span class="tt">[<span class="t0">action, feature, detection, aware, approach, human, training</span>] [<span class="t1">data, visual, image, calculated, camera, gaussian</span>] [<span class="t2">recognition, natural, video, spatial, precision, sparse, test</span>] [<span class="t3">number, detected, measure, relative, additional, correct</span>] [<span class="t4">interest, depth, point, descriptor, saliency, rmd, bag, equation, hessian, standard, extended, including, motion, average, threshold, dataset, performed, hollywood, local, histogram, large, salient, extracted, provided, separable, table, laptev, dense, stream, provide, order, paper, greater, content, sampling, background, integral, datasets, prove, better</span>] [<span class="t5">harris, based, intensity, temporal, performance, range, combination</span>] [<span class="t6">appearance, work, example, state, structural, well</span>] </span>
	</div>



	<div class="apaper" id="pid15">
	<div class="paperdesc">
		<span class="ts">Better Exploiting Motion for Better Action Recognition</span><br />
		<span class="as">Mihir Jain, Herv Jgou, Patrick Bouthemy</span><br />
		<span class="keywords">Keywords: action recognition, motion compensation, affine motion, kinematic features, VLAD</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Jain_Better_Exploiting_Motion_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim15">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab159565132">[abstract]</span>-->
	</div>
	<img src = "thumbs/Jain_Better_Exploiting_Motion_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder159565132"></div>-->
	<span class="tt">[<span class="t0">action, human, model, feature, approach, accuracy, bof, figure, art, svm</span>] [<span class="t1">camera, image, consider, visual, corresponding, existing, note, horizontal</span>] [<span class="t2">recognition, video, optical, dominant, volume, proposed, global, residual, codebook</span>] [<span class="t3">combining, additional</span>] [<span class="t4">motion, descriptor, local, computed, vlad, table, hof, trajectory, compensated, kinematic, mbh, impact, computation, vector, hog, technique, large, better, compensation, dense, point, dataset, improves, improvement, curl, average, compute, complementary, shear, background, scalar, olympic, paper, best, interest, sake, introduced, involved</span>] [<span class="t5">encoding, performance, combination, reported, encode, representation, scheme, based</span>] [<span class="t6">tracking, method, set, state, moving, introduce, pattern, map</span>] </span>
	</div>



	<div class="apaper" id="pid16">
	<div class="paperdesc">
		<span class="ts">Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera</span><br />
		<span class="as">Lu Xia, J.K. Aggarwal</span><br />
		<span class="keywords">Keywords: Spatio temporal interest point, depth image, activity recognition, Kinect</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Xia_Spatio-temporal_Depth_Cuboid_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim16">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab886194254">[abstract]</span>-->
	</div>
	<img src = "thumbs/Xia_Spatio-temporal_Depth_Cuboid_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder886194254"></div>-->
	<span class="tt">[<span class="t0">feature, action, human, figure, accuracy, joint, denotes, body, detection, training</span>] [<span class="t1">algorithm, image, location, extract, data, position, existing</span>] [<span class="t2">recognition, video, pixel, spatial, volume, test</span>] [<span class="t3">number, function, similarity, describe, experimental, second</span>] [<span class="t4">depth, cuboid, local, rgb, interest, noise, dstip, dcsf, descriptor, nxy, size, point, dataset, histogram, better, correction, dstips, real, scale, skeleton, posture, skeletal, appear, table, extracted, background, voxels, magnitude, occupancy, movement, extraction, adaptable, comparison, average</span>] [<span class="t5">detector, stips, based, temporal, stip, signal</span>] [<span class="t6">activity, work, response, computer, object, method, category, build, vision</span>] </span>
	</div>



	<div class="apaper" id="pid17">
	<div class="paperdesc">
		<span class="ts">Recognize Human Activities from Partially Observed Videos</span><br />
		<span class="as">Yu Cao, Daniel Barrett, Andrei Barbu, Siddharth Narayanaswamy, Haonan Yu, Aaron Michaux, Yuewei Lin, Sven Dickinson, Jeffrey Mark Siskind, Song Wang</span><br />
		<span class="keywords">Keywords: </span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Cao_Recognize_Human_Activities_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim17">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab855979088">[abstract]</span>-->
	</div>
	<img src = "thumbs/Cao_Recognize_Human_Activities_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder855979088"></div>-->
	<span class="tt">[<span class="t0">training, human, feature, action, trained, spatiotemporal, class, accuracy, figure, evaluated, overlap</span>] [<span class="t1">problem, multiple, cross</span>] [<span class="t2">video, darpa, recognition, test, observed, mssc, proposed, baseline, sparse, prediction, segment, likelihood, coding, partially, gap, gapfilliing, ratio, recall, mmed, precision, gapfilling, subsequence, long, posterior, length, natural, achieve, short, dynamic, fully, general, evaluate, constructed, construct, mixture, early, presented, unobserved, special, handle, practical, studied</span>] [<span class="t3">observation, three, number, duration, eye, stage</span>] [<span class="t4">better, datasets, comparison, complex, dataset, average, interest, large, vector</span>] [<span class="t5">temporal, performance, evaluation, include, perform, event, full</span>] [<span class="t6">activity, method, frame, recognizing, set, computer, recognized, program</span>] </span>
	</div>



	<div class="apaper" id="pid18">
	<div class="paperdesc">
		<span class="ts">Unconstrained Monocular 3D Human Pose Estimation by Action Detection and Cross-Modality Regression Forest</span><br />
		<span class="as">Tsz-Ho Yu, Tae-Kyun Kim, Roberto Cipolla</span><br />
		<span class="keywords">Keywords: Human Pose Estimation, Random Forest, Hough Forest, Regression Forest</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yu_Unconstrained_Monocular_3D_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim18">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab959093458">[abstract]</span>-->
	</div>
	<img src = "thumbs/Yu_Unconstrained_Monocular_3D_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder959093458"></div>-->
	<span class="tt">[<span class="t0">pose, action, human, estimation, detection, regression, joint, forest, wave, training, class, model, feature, body, figure, localisation, ape, accuracy, hpe, dpm, approach, monocular, box, trained, unconstrained, bend, split, articulated, clap, deformable, dance, feasibility, evaluated, current, pictorial, challenging, error, spatiotemporal, knowledge, tree, balance, leaf</span>] [<span class="t1">estimate, image, data, framework, estimated, multiple, ground, infer, corresponding, sample</span>] [<span class="t2">proposed, recognition, structure, video, simultaneously</span>] [<span class="t3">testing, learning, detected, time, kinect, input, high</span>] [<span class="t4">dataset, depth, vector, performed, motion, combined, background, table</span>] [<span class="t5">truth</span>] [<span class="t6">method, node, frame, hand, computer</span>] </span>
	</div>



	<div class="apaper" id="pid19">
	<div class="paperdesc">
		<span class="ts">Detection of Manipulation Action Consequences (MAC)</span><br />
		<span class="as">Yezhou Yang, Cornelia Fermller, Yiannis Aloimonos</span><br />
		<span class="keywords">Keywords: </span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yang_Detection_of_Manipulation_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim19">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab20370003">[abstract]</span>-->
	</div>
	<img src = "thumbs/Yang_Detection_of_Manipulation_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder20370003"></div>-->
	<span class="tt">[<span class="t0">action, human, model, figure, detection, distribution</span>] [<span class="t1">visual, algorithm, sample, image, target, robust, problem, consider</span>] [<span class="t2">recognition, optical, video, understanding, process, volume, natural, case, proposed, represent, pixel, detecting, segment</span>] [<span class="t3">system, sequence, time, semantic, number</span>] [<span class="t4">point, movement, dataset, motion, sampling, described, depth, provide, order</span>] [<span class="t5">color, based, term, ability, performance</span>] [<span class="t6">segmentation, object, method, tracking, manipulation, set, active, graph, appearance, consequence, tracked, goal, condition, attention, vision, computer, weighted, edge, area, cut, vsg, division, change, intelligent, mac, monitor, initial, stochastic, example, weight, topological, assemble, denote, international, work, monitoring, fundamental, represents, crucial, considered</span>] </span>
	</div>



	<div class="apaper" id="pid20">
	<div class="paperdesc">
		<span class="ts">First-Person Activity Recognition: What Are They Doing to Me?</span><br />
		<span class="as">Michael S. Ryoo, Larry Matthies</span><br />
		<span class="keywords">Keywords: human activity recognition, first-person computer vision</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Ryoo_First-Person_Activity_Recognition_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim20">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab106986074">[abstract]</span>-->
	</div>
	<img src = "thumbs/Ryoo_First-Person_Activity_Recognition_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder106986074"></div>-->
	<span class="tt">[<span class="t0">human, figure, training, approach, detection, feature, wave, pyramid</span>] [<span class="t1">optimal, continuous, multiple, camera, visual, target, matrix</span>] [<span class="t2">structure, kernel, video, recognition, global, match, optical, hug, pet, observer, advantage, superior, alignment, baseline, previous, represent, punch, robot, shake, observed, constructed, humanoid, throw, version, punching, idea, measured, subsection, learned, arg, considers, evaluate</span>] [<span class="t3">system, learning, function, number, hierarchical, time, detected, matching, positive</span>] [<span class="t4">motion, local, histogram, dataset, paper, order, better, average, best, descriptor, computed, point</span>] [<span class="t5">temporal, gram, evaluation, representation, performance, based, result, ability</span>] [<span class="t6">activity, distance, example, person, recognizing, recognize</span>] </span>
	</div>



	<div class="apaper" id="pid21">
	<div class="paperdesc">
		<span class="ts">Finding Group Interactions in Social Clutter</span><br />
		<span class="as">Ruonan Li, Parker Porfilio, Todd Zickler</span><br />
		<span class="keywords">Keywords: </span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Finding_Group_Interactions_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim21">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab475982174">[abstract]</span>-->
	</div>
	<img src = "thumbs/Li_Finding_Group_Interactions_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder475982174"></div>-->
	<span class="tt">[<span class="t0">approach, detection, localization, action, figure, accuracy, bounding</span>] [<span class="t1">optimal, consider, matrix, problem, labeled, framework, single</span>] [<span class="t2">video, detecting, recognition, long, ensemble, proposed, evaluate, represent</span>] [<span class="t3">interaction, matching, exemplar, time, pairwise, input, group, individual, social, three, unit, agent, number, false, detected, system, localizing, positive, occur, small, relative, learning, similarity, collection, annotated, measure, distinctive, larger, matched, instantaneous, localized, neighborhood, wnm, voting, simply, lowest</span>] [<span class="t4">descriptor, best, compute, large, table, report, described, database, dataset, average</span>] [<span class="t5">temporal, metric, performance, representation, based, selected</span>] [<span class="t6">interval, search, tracking, category, set</span>] </span>
	</div>



	<div class="apaper" id="pid22">
	<div class="paperdesc">
		<span class="ts">Online Dominant and Anomalous Behavior Detection in Videos</span><br />
		<span class="as">Mehrsan Javan Roshtkhari, Martin D. Levine</span><br />
		<span class="keywords">Keywords: Surveillance, Video parsing, Hierarchical scene modeling, Behavior learning, Anomaly detection, Bag of video words, Contextual information, Spatio-Temporal compositions</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Roshtkhari_Online_Dominant_and_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim22">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab276006160">[abstract]</span>-->
	</div>
	<img src = "thumbs/Roshtkhari_Online_Dominant_and_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder276006160"></div>-->
	<span class="tt">[<span class="t0">detection, approach, model, figure, space, spatiotemporal, template</span>] [<span class="t1">algorithm, consider, represented, sample, data, learnt</span>] [<span class="t2">video, dominant, spatial, behavior, scene, pixel, abnormal, ensemble, stvs, detecting, online, abnormality, clustering, anomalous, understanding, codebook, construct, oriented, proposed, volume, optical, contextual, capable, behaviour, achieved, pdf, cluster, fuzzy, probabilistic, employed, anomaly, dynamic, simultaneously, observed, utkt, usks, rare, constructed, employing, bov</span>] [<span class="t3">level, similarity, hierarchical, learning, number, rate, detect, analysis, topic</span>] [<span class="t4">local, background, dataset, order, gradient, described, large, descriptor, performed</span>] [<span class="t5">temporal, based, surveillance</span>] [<span class="t6">method, set, activity, pattern, frame, change</span>] </span>
	</div>



	<div class="apaper" id="pid23">
	<div class="paperdesc">
		<span class="ts">Cross-View Action Recognition via a Continuous Virtual Path</span><br />
		<span class="as">Zhong Zhang, Chunheng Wang, Baihua Xiao, Wen Zhou, Shuang Liu, Cunzhao Shi</span><br />
		<span class="keywords">Keywords: </span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Zhang_Cross-View_Action_Recognition_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim23">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab744342757">[abstract]</span>-->
	</div>
	<img src = "thumbs/Zhang_Cross-View_Action_Recognition_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder744342757"></div>-->
	<span class="tt">[<span class="t0">action, feature, accuracy, correspondence, class, approach, human, training, discriminative, propose, achieves</span>] [<span class="t1">virtual, view, target, labeled, unlabeled, source, path, corresponding, constraint, vvkc, algorithm, multiple, continuous, transformation, domain, ixmas, problem, mode, matrix, robust, discrimination, vvk, maximize, inner, parameter, max, rotation, extract, linear, maximizing, theoretic, gaussian, working, sample, approximated, objective, kau, kat</span>] [<span class="t2">recognition, kernel, partially, proposed, construct, observed, video, learned</span>] [<span class="t3">similarity, three, learning</span>] [<span class="t4">table, better, average, computed, interest, compute, dataset, local, sampling, vector, equation</span>] [<span class="t5">representation, temporal, performance, based, key</span>] [<span class="t6">method, set, work, recognizing, change, pattern</span>] </span>
	</div>



	<div class="apaper" id="pid24">
	<div class="paperdesc">
		<span class="ts">Event Retrieval in Large Video Collections with Circulant Temporal Encoding</span><br />
		<span class="as">Jrme Revaud, Matthijs Douze, Cordelia Schmid, Herv Jgou</span><br />
		<span class="keywords">Keywords: large-scale, video retrieval, event retrieval</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Revaud_Event_Retrieval_in_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim24">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab66408316">[abstract]</span>-->
	</div>
	<img src = "thumbs/Revaud_Event_Retrieval_in_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder66408316"></div>-->
	<span class="tt">[<span class="t0">detection, approach, figure, pca</span>] [<span class="t1">image, web, domain, single, assumption, visual</span>] [<span class="t2">video, alignment, task, length, proposed, case, presented</span>] [<span class="t3">time, matching, number, similarity, complexity, sequence, small, description, independent</span>] [<span class="t4">database, query, cte, retrieval, fourier, vector, mmv, copy, complex, descriptor, evve, dataset, table, recvid, large, product, frequency, technique, circulant, memory, regularization, paper, concert, quantization, applied, comparison, performed, average, compute, inverse, william, ndcr, wedding, described, shift, averaging, kate, hough, compressed, distractors, size, mapped, compression</span>] [<span class="t5">event, temporal, performance, evaluation, encoding, representation, metric, partial</span>] [<span class="t6">frame, search, method, set, requires, state, compare, well</span>] </span>
	</div>



	<div class="apaper" id="pid25">
	<div class="paperdesc">
		<span class="ts">Action Recognition by Hierarchical Sequence Summarization</span><br />
		<span class="as">Yale Song, Louis-Philippe Morency, Randall Davis</span><br />
		<span class="keywords">Keywords: Action Recognition, Hierarchical Model, Conditional Random Fields</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Song_Action_Recognition_by_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim25">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab463371246">[abstract]</span>-->
	</div>
	<img src = "thumbs/Song_Action_Recognition_by_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder463371246"></div>-->
	<span class="tt">[<span class="t0">feature, approach, model, action, discriminative, human, figure, training, body, accuracy, joint</span>] [<span class="t1">algorithm, learn, single, data, multiple, optimal, parameter, optimization</span>] [<span class="t2">recognition, length, posterior, previous</span>] [<span class="t3">sequence, learning, hierarchical, latent, summarization, layer, function, observation, gate, super, log, number, grouping, hierarchy, hidden, procedure, deep, armgesture, complexity, similarity, difference, group, natops, experimental, conditional, semantic, three, analysis, variable, wst, second, belief, testing, learns, sublinearly, neural, input, abstract, incremental</span>] [<span class="t4">dataset, local, datasets, equation, order, complex, average, vector</span>] [<span class="t5">representation, temporal, summary, based, performance, result, term</span>] [<span class="t6">set, activity, work, solution, frame, label, original</span>] </span>
	</div>



	<div class="apaper" id="pid26">
	<div class="paperdesc">
		<span class="ts">Spatiotemporal Deformable Part Models for Action Detection</span><br />
		<span class="as">Yicong Tian, Rahul Sukthankar, Mubarak Shah</span><br />
		<span class="keywords">Keywords: </span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Tian_Spatiotemporal_Deformable_Part_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim26">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab587247345">[abstract]</span>-->
	</div>
	<img src = "thumbs/Tian_Spatiotemporal_Deformable_Part_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder587247345"></div>-->
	<span class="tt">[<span class="t0">action, sdpm, detection, spatiotemporal, root, model, deformable, feature, space, cycle, bounding, localization, dpm, ucf, capture, score, volumetric, employ, training, human, variation, ith, pyramid, discriminative, figure, svm, template, box, weizmann, overlap, trained, select, achieves, approach, enable, cell, dpms, automatically</span>] [<span class="t1">corresponding, single</span>] [<span class="t2">video, volume, recognition, learned, test, structure, observed, clip, evaluate, spatial, global</span>] [<span class="t3">number, level, learning, small, positive, time, determined, detect</span>] [<span class="t4">dataset, descriptor, complex, background, large, motion, computed, better, standard, best</span>] [<span class="t5">temporal, selected, performance</span>] [<span class="t6">object, set, train, deformation, appearance, search, example, frame, well</span>] </span>
	</div>



	<div class="apaper" id="pid27">
	<div class="paperdesc">
		<span class="ts">Sampling Strategies for Real-Time Action Recognition</span><br />
		<span class="as">Feng Shi, Emil Petriu, Robert Laganire</span><br />
		<span class="keywords">Keywords: </span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Shi_Sampling_Strategies_for_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim27">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab971162098">[abstract]</span>-->
	</div>
	<img src = "thumbs/Shi_Sampling_Strategies_for_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder971162098"></div>-->
	<span class="tt">[<span class="t0">action, feature, root, model, human, kth, approach, accuracy, patch, training, select, challenging, discriminative, realistic</span>] [<span class="t1">visual, image</span>] [<span class="t2">video, recognition, spatial, kernel, intersection, observed, proposed, structure</span>] [<span class="t3">number, high, three, eye</span>] [<span class="t4">sampling, dense, local, random, sampled, size, table, integral, average, interest, resolution, descriptor, point, background, dataset, half, dimension, speed, density, computation, large, better, grid, histogram, standard, mbh, computationally, report, computational, datasets, codewords, hof, gpu, best, cpu, factor</span>] [<span class="t5">performance, randomly, based, full, total, temporal, uniform, good</span>] [<span class="t6">method, computer, vision, object, set</span>] </span>
	</div>



	<div class="apaper" id="pid28">
	<div class="paperdesc">
		<span class="ts">Bilinear Programming for Human Activity Recognition with Unknown MRF Graphs</span><br />
		<span class="as">Zhenhua Wang, Qinfeng Shi, Chunhua Shen, Anton van_den_Hengel</span><br />
		<span class="keywords">Keywords: </span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wang_Bilinear_Programming_for_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim28">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab459679095">[abstract]</span>-->
	</div>
	<img src = "thumbs/Wang_Bilinear_Programming_for_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder459679095"></div>-->
	<span class="tt">[<span class="t0">human, model, figure, body, error, feature, joint, discriminative, training, svm, action</span>] [<span class="t1">problem, solve, consider, solving, image, data, coordinate, max, estimate</span>] [<span class="t2">recognition, video, global, prediction</span>] [<span class="t3">function, relative, learning, group, predicting, second, belief, degree, latent, observation</span>] [<span class="t4">local, dataset, datasets, vector, descriptor, outperforms, random, table</span>] [<span class="t5">based, energy</span>] [<span class="t6">graph, method, bilinear, map, activity, blp, inference, mrf, unknown, program, relaxation, label, branch, bound, solution, min, node, pattern, person, mcsvm, qinit, mrfs, set, badminton, edge, har, ssvm, predict, structural, green, confusion, ascent, tennis, optimisation, synthetic, apply, work, style, isolate, vision, computer</span>] </span>
	</div>



	<div class="apaper" id="pid29">
	<div class="paperdesc">
		<span class="ts">Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition</span><br />
		<span class="as">Vinay Bettadapura, Grant Schindler, Thomas Ploetz, Irfan Essa</span><br />
		<span class="keywords">Keywords: Activity Recognition, Anomaly Detection, Bag of Words, Skill Assessment, Surveillance, Video</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Bettadapura_Augmenting_Bag-of-Words_Data-Driven_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim29">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab697522040">[abstract]</span>-->
	</div>
	<img src = "thumbs/Bettadapura_Augmenting_Bag-of-Words_Data-Driven_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder697522040"></div>-->
	<span class="tt">[<span class="t0">figure, human, approach, model, space, training, action, capture, pyramid</span>] [<span class="t1">data</span>] [<span class="t2">recognition, video, baseline, structure, global, behavior, represent, cluster</span>] [<span class="t3">time, sequence, learning, second, duration, three, analysis, semantic</span>] [<span class="t4">local, standard, table, dataset, histogram, order, random, complex, datasets, vector</span>] [<span class="t5">bow, temporal, event, observable, encoding, augmented, regular, based, randomly, elapsed, sequential, skill, quantized, assessment, soccer, unsupervised, interspersed, surveillance, surgical, representation, statistical, evaluation, ordering, automatic, consecutive, modeling, ocean, generated, waas, total, main, player, outperform, cumulative, underlying</span>] [<span class="t6">activity, set, structural, recognizing, method, vehicle, augment, example</span>] </span>
	</div>



	<div class="apaper" id="pid30">
	<div class="paperdesc">
		<span class="ts">A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</span><br />
		<span class="as">Pradipto Das, Chenliang Xu, Richard F. Doell, Jason J. Corso</span><br />
		<span class="keywords">Keywords: video to text, video understanding, multimodal topic model, natural language</span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Das_A_Thousand_Frames_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim30">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab537645052">[abstract]</span>-->
	</div>
	<img src = "thumbs/Das_A_Thousand_Frames_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder537645052"></div>-->
	<span class="tt">[<span class="t0">human, model, training, template, detection, action, feature, figure, approach</span>] [<span class="t1">visual, image, data</span>] [<span class="t2">video, natural, test, recall, precision, short</span>] [<span class="t3">topic, concept, level, language, lingual, low, description, system, man, number, middle, semantic, woman, top, cooking, keywords, sentence, high, output, latent, text, tripartite, group, mmlda, rock, relevant, generating, lower, predicted, rouge, trecvid, bacon, town, word, hall, pan, three, metal, annotation, small, corpus, ranked</span>] [<span class="t4">dataset, table, large, average</span>] [<span class="t5">event, based, color, evaluation</span>] [<span class="t6">set, object, person, method, graph, work, vision, computer</span>] </span>
	</div>



	<div class="apaper" id="pid31">
	<div class="paperdesc">
		<span class="ts">3D R Transform on Spatio-temporal Interest Points for Action Recognition</span><br />
		<span class="as">Chunfeng Yuan, Xi Li, Weiming Hu, Haibin Ling, Stephen Maybank</span><br />
		<span class="keywords">Keywords: </span><br /><br />
	</div>
	<div class="dllinks">
		<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Yuan_3D_R_Transform_2013_CVPR_paper.pdf">[pdf] </a>
		<span class="sim" id="sim31">[rank by tf-idf similarity to this]</span><br />
		<!--<span class="abstr" id="ab364295679">[abstract]</span>-->
	</div>
	<img src = "thumbs/Yuan_3D_R_Transform_2013_CVPR_paper.pdf.jpg"><br />
	<!--<div class = "abstrholder" id="abholder364295679"></div>-->
	<span class="tt">[<span class="t0">feature, transform, fusion, action, bovw, kth, radon, distribution, approach, sin, pca, ucf, geometrical, figure, propose, human, accuracy, svm, achieves, spatiotemporal, employ, model, aware, denotes, supported, discriminative, science</span>] [<span class="t1">matrix, visual, parameter, effectiveness, robust, position</span>] [<span class="t2">kernel, video, recognition, proposed, global, calculation, contextual, combine, test, codebook, achieve, represent, referred</span>] [<span class="t3">similarity, discrete, number, detected, pairwise, second, function, describe, nearest</span>] [<span class="t4">interest, local, point, average, higher, table, computed, compute, dataset, extracted, vector, cuboid</span>] [<span class="t5">based, representation, evaluation, form, invariant, performance, perform</span>] [<span class="t6">context, method, object, set, apply, computer, normalized, work</span>] </span>
	</div>


</div>

</div>

<br /><br /><br /><br /><br /><br />
</body>

</html>
